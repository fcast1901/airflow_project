from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.datasets import Dataset
from functions.on_failure_callback import task_fail_slack_alert


import requests
import json
from datetime import datetime
import pandas as pd


##################################################################
#Hooks
##################################################################

postgres_hook = PostgresHook(postgres_conn_id="postgres")


##################################################################
#Tasks
##################################################################

def _collect_climate_data():
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    #create table if not exists
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS clima.climate_data (
            id SERIAL PRIMARY KEY,
            zip INT,
            lat FLOAT,
            lng FLOAT,
            api_call_date DATE,
            data JSONB
        );
    """)
    conn.commit()
    #query the coordinates
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT 
            zip,
            lat,
            lng 
        FROM clima.zip_coordinates;
    """)
    data = cursor.fetchall()
    df = pd.DataFrame(data, columns = ['zip', 'lat', 'lng'])
    cursor.execute("""
        SELECT zip
        FROM clima.climate_data
        WHERE api_call_date = CURRENT_DATE;
    """)
    existing_zips = cursor.fetchall()
    existing_zips = [row[0] for row in existing_zips]
    print(existing_zips)
    df['zip'] = df['zip'].astype(int)
    df = df[~df['zip'].isin(existing_zips)]
    columns = ['zip', 'lat', 'lng', 'api_call_date', 'data']
    hist_df = pd.DataFrame(columns=columns)
    for row in df.itertuples():
        lat = row.lat
        long = row.lng
        response = requests.get(f'https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={long}&daily=temperature_2m_max,temperature_2m_min,precipitation_sum,snowfall_sum,uv_index_max')
        response_json = response.json()
        new_row = pd.DataFrame([{'zip': row.zip, 
                   'lat': row.lat, 
                   'lng': row.lng, 
                   'api_call_date': datetime.now().strftime('%Y-%m-%d'), 
                   'data': json.dumps(response_json)}])
        hist_df = pd.concat([hist_df, new_row], ignore_index=True)
    #store hist_df as csv
    hist_df.to_csv('/tmp/hist_df.csv', index=False)
    cursor.close()
    conn.close()
    return hist_df

def _insert_data_to_postgres():
    #create table if not exists with an autogenerated key
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    #store hist_df in postgres
    postgres_hook.copy_expert(
                sql="""
                    COPY clima.climate_data (zip, lat, lng, api_call_date, data)
                    FROM STDIN WITH CSV HEADER DELIMITER AS ','
                """,
                filename="/tmp/hist_df.csv",
    )
    conn.commit()
    cursor.close()
    conn.close()
    
def dataset_function():
        pass

with DAG(
    dag_id="historic_data_dag",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily",
    catchup=False,
    tags=["climate", "daily", "historic"],
):
    dataset_task = PythonOperator(
        task_id="dataset_task",
        python_callable=dataset_function,
        outlets=[Dataset("/tmp/dataset.csv")]
    )

    start_task = EmptyOperator(task_id="start_task")

    end_task = EmptyOperator(task_id="end_task")

    collect_climate_data = PythonOperator(
        task_id="collect_climate_data",
        python_callable=_collect_climate_data,
        on_failure_callback=task_fail_slack_alert,
    )

    insert_data_to_postgres = PythonOperator(
        task_id="insert_data_to_postgres",
        python_callable=_insert_data_to_postgres,
        on_failure_callback=task_fail_slack_alert,
    )


start_task >> collect_climate_data >> insert_data_to_postgres >> dataset_task >> end_task
