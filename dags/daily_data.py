from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.datasets import Dataset
from airflow.operators.empty import EmptyOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.task_group import TaskGroup
from functions.on_failure_callback import task_fail_slack_alert


import json
from datetime import datetime
import pandas as pd


##################################################################
#Hooks
##################################################################

postgres_hook = PostgresHook(postgres_conn_id="postgres")


##################################################################
#Tasks
##################################################################
def _collect_climate_data():
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT
            id, 
            zip,
            lat,
            lng,
            api_call_date,
            data 
        FROM clima.climate_data
        WHERE api_call_date = CURRENT_DATE;
    """)
    data = cursor.fetchall()
    hist_df = pd.DataFrame(data, columns = ['id','zip', 'lat', 'lng', 'api_call_date', 'data'])
    hist_df.to_csv('/tmp/hist_df.csv', index=False)
    cursor.close()
    conn.close()
    return hist_df

def _process_data():
    hist_df = pd.read_csv('/tmp/hist_df.csv')
    processed_data = []

    for index, row in hist_df.iterrows():
        try:
            # Sanitize the JSON string in case of single quotes or other issues
            sanitized_data = row['data'].replace("'", '"')  # Replace single quotes with double quotes
            climate_data = json.loads(sanitized_data)  # Parse the JSON
            
            daily_data = climate_data['daily']
            for i in range(len(daily_data['time'])):
                processed_data.append({
                    'zip': row['zip'],
                    'time': daily_data['time'][i],
                    'lat': row['lat'],
                    'lng': row['lng'],
                    'temperature_2m_max (Â°C)': daily_data['temperature_2m_max'][i],
                    'temperature_2m_min (Â°C)': daily_data['temperature_2m_min'][i],
                    'uv_index_max ()': daily_data['uv_index_max'][i],
                    'precipitation_sum (mm)': daily_data['precipitation_sum'][i],
                    'snowfall_sum (cm)': daily_data['snowfall_sum'][i]
                })
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
        except KeyError as e:
            print(f"Missing key in JSON for row {index}: {e}")

    # Convert to a DataFrame
    output_df = pd.DataFrame(processed_data)
    print(output_df)
    output_df.to_csv('/tmp/processed_climate_data.csv', index=False)
    return output_df

def _truncate_table():
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT EXISTS (
            SELECT 1 
            FROM information_schema.tables 
            WHERE table_name = 'last_weekly_prediction'
        );
    """)
    table_exists = cursor.fetchone()[0]
    if table_exists > 0:
        cursor.execute("TRUNCATE TABLE clima.last_weekly_prediction;")
        print('Table truncated')
    else:
        print('Table does not exist')
        #create table if not exists
        #zip as a priomary key to be able to update the table (SCD type 1 logic)
        cursor.execute("""CREATE TABLE IF NOT EXISTS clima.last_weekly_prediction (
            zip INT,
            time DATE,
            lat FLOAT,
            lng FLOAT,
            temperature_2m_max FLOAT,
            temperature_2m_min FLOAT,
            precipitation_sum FLOAT,
            UVI FLOAT,
            snowfall_sum FLOAT,
            PRIMARY KEY (zip, time)       
        );""")
    conn.commit()
    cursor.close()
    conn.close()

def _insert_data_to_postgres():
    conn = postgres_hook.get_conn()
    cursor = conn.cursor()
    #store hist_df in postgres
    postgres_hook.copy_expert(
                sql="""
                    COPY clima.last_weekly_prediction (zip, time, lat, lng, temperature_2m_max, temperature_2m_min, precipitation_sum, UVI, snowfall_sum)
                    FROM STDIN WITH CSV HEADER DELIMITER AS ','
                """,
                filename="/tmp/processed_climate_data.csv",
    )
    conn.commit()
    cursor.close()
    conn.close()
#https://www.astronomer.io/docs/learn/airflow-datasets/?tab=traditional#basic-dataset-schedule-definition
#we use a dataset generated by the historic data dag to launch this dag once the other is executed
#this is a way to create a dependency between dags
#we query the daily information, process it and load it into a table

with DAG(
    dag_id="daily_dag",
    start_date=datetime(2024, 1, 1),
    schedule=[Dataset("/tmp/dataset.csv")],
    catchup=False,
    tags=["climate", "daily"],
):
    start_task = EmptyOperator(task_id="start_task")

    end_task = EmptyOperator(task_id="end_task")

    collect_climate_data = PythonOperator(
        task_id="collect_climate_data",
        python_callable=_collect_climate_data,
        on_failure_callback=task_fail_slack_alert
    )

    # TaskGroup for data tasks (better organization)

    with TaskGroup("data_processing_tasks", tooltip="Tasks for data processing") as data_processing_tasks:
        process_data = PythonOperator(
            task_id="process_data",
            python_callable=_process_data,
            on_failure_callback=task_fail_slack_alert,
        )

        truncate_table = PythonOperator(
            task_id="truncate_table",
            python_callable=_truncate_table,
            on_failure_callback=task_fail_slack_alert,
        )

        insert_data_to_postgres = PythonOperator(
            task_id="insert_data_to_postgres",
            python_callable=_insert_data_to_postgres,
            on_failure_callback=task_fail_slack_alert,
        )

        process_data >> truncate_table >> insert_data_to_postgres

# Define the rest of the DAG
start_task >> collect_climate_data >> data_processing_tasks >> end_task